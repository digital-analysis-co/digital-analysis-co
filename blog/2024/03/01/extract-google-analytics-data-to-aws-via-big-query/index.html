
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
        <link rel="next" href="../../../12/01/google-analytics-not-set-landing-pages/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Extract Google Analytics Data to AWS (Via Big Query) - Digital Analysis Co</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-VCSWSYFRX3"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-VCSWSYFRX3",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-VCSWSYFRX3",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#extract-google-analytics-data-to-aws-via-big-query" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Digital Analysis Co" class="md-header__button md-logo" aria-label="Digital Analysis Co" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Digital Analysis Co
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Extract Google Analytics Data to AWS (Via Big Query)
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Digital Analysis Co" class="md-nav__button md-logo" aria-label="Digital Analysis Co" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Digital Analysis Co
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../resume/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Resume
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Blog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inputs-needed-to-follow-this-post" class="md-nav__link">
    <span class="md-ellipsis">
      Inputs Needed to Follow this Post
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#create-a-pub-sub-topic-in-google-cloud" class="md-nav__link">
    <span class="md-ellipsis">
      Create a Pub / Sub Topic in Google Cloud
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-a-cloud-function-to-trigger-a-github-actions-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Use a Cloud Function to Trigger a Github Actions Workflow
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rsync-data-between-google-storage-and-s3" class="md-nav__link">
    <span class="md-ellipsis">
      Rsync data between Google Storage and S3
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-03-01 00:00:00+00:00" class="md-ellipsis">March 1, 2024</time>
                      </div>
                    </li>
                    
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              10 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        


<h1 id="extract-google-analytics-data-to-aws-via-big-query">Extract Google Analytics Data to AWS (Via Big Query)</h1>
<h2 id="summary">Summary</h2>
<p>GA4 provides a convenient point and click extract into Big Query. On several occassions I've been asked to forward this extract to AWS. <!-- more --> GA4 data in the Big Query extract are retained for a rolling 60 days. <a href="https://support.google.com/analytics/answer/9358801?hl=en">Details</a>.</p>
<blockquote>
<p>You can export to a free instance of BigQuery <a href="https://cloud.google.com/bigquery/docs/sandbox">(BigQuery sandbox)</a>, but exports that exceed the sandbox limits incur charges</p>
</blockquote>
<p>The Big Query extract is useful because all GA4 data are already neatly packaged up into an event fact table. If using the API directly you would need to wrestle with limits and restrictions on dimension and metric combinations. </p>
<p><a href="https://support.google.com/analytics/answer/9358801?hl=en">GA4 Big Query Export Documentation</a></p>
<p>With the Big Query extract up and running, you might want to get your GA4 data into AWS, Azure or anywhere else.</p>
<p>The gist of this post is exporting GA4 data into Google Storage (GS) whenever extract data are updated, then <a href="https://cloud.google.com/storage/docs/gsutil/commands/rsync"><code>Rsync</code>ing</a> between GS and AWS S3.</p>
<p>"whenever extract data are updated". This part <a href="https://www.teamsimmer.com/2022/12/07/how-do-i-trigger-a-scheduled-query-when-the-ga4-daily-export-happens/">Draws heavily from Simo's Simmer post "How Do I Trigger A Scheduled Query When The GA4 Daily Export Happens"</a></p>
<p>GA4 data are extracted at inconsistent times and daily tables can be updated up to 72 hours or longer after the date of the table (See linked documentation above for details).</p>
<p>I used Github Actions (GHA) with this workflow but you could keep it all in a cloud function if you prefer. I chose Github Actions here because:
 * In addition to running the AWS extract automatically on each update, you can also back fill over a custom date range using user input fields in the Actions tab of your repo. The actions tab is a convenient UI.
 * The GHA community actions and marketplace make it very easy and convenient to use additional tools in your workflow, in this case the Google Cloud cli. The AWS cli is already pre-installed on the runners.
 * You might want to trigger other workflows after GA4 data have been ingested, such as running some DBT models.</p>
<p>Approach:
1. (Google Cloud) Create a Pub/Sub topic using the Logs Router which listens for GA4 daily extract updates and is used to trigger a cloud function and pass details to the function, including the extract event date for the data.
2. (Google Cloud) Use a Cloud Function to trigger a workflow in Github Actions using a webhook.
3. (Github Actions) Extract the table's event data into Google Storage using the Google Cloud CLI.
4. (Github Actions) Rsync data from Google Storage to AWS S3</p>
<p>Once data are in S3 you can then populate a table in your preferred SQL engine, such as Redshift or Athena. The Big Query extract contains structured data, just create an equivalent table with like field data types to those of <a href="https://support.google.com/analytics/answer/7029846?hl=en">the Big Query one</a>. Or, if using Redshift, copy / paste the <code>create table</code> sql below.</p>
<h2 id="inputs-needed-to-follow-this-post">Inputs Needed to Follow this Post</h2>
<ul>
<li>A Github repo along with a fine grained access token (details below). This repo is used to run an actions workflow that moves GA4 data from BQ to AWS. Ensure the repo has actions enabled.</li>
<li>GA4 daily extract to Big Query is up and running. When you first set this up it will take 1-2 days before data start coming in. <a href="https://support.google.com/analytics/answer/9823238">Documentation</a>.</li>
<li>Json for a Google service account to allow Github Actions to access GA4 data on Google Cloud and to temporarily move it to Google Storage</li>
<li>AWS S3 credentials for a bucket to send the data to, including <code>AWS_S3_BUCKET</code>, <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_DEFAULT_REGION</code>.</li>
<li>A Google Storage bucket to save the data in before sending to AWS S3.</li>
</ul>
<h2 id="create-a-pub-sub-topic-in-google-cloud">Create a Pub / Sub Topic in Google Cloud</h2>
<p>In short, we want to create a pub / sub topic using the <a href="https://console.cloud.google.com/logs/router">Logs Router</a> in Google Cloud. This will allow us to trigger a Cloud Function when GA4 data are extracted into BQ.</p>
<p>As mentioned above, I drew on <a href="https://www.teamsimmer.com/2022/12/07/how-do-i-trigger-a-scheduled-query-when-the-ga4-daily-export-happens/">the article by Simo at Simmer</a> to complete this step. See the link for more details, especially on how to filter logs for GA4 extract updates under the section "2. Create the Logs Router".</p>
<h2 id="use-a-cloud-function-to-trigger-a-github-actions-workflow">Use a Cloud Function to Trigger a Github Actions Workflow</h2>
<p>Once the Pub / Sub sink has been created per the previous step, create a <a href="https://console.cloud.google.com/functions/list">Cloud Function</a> and for the trigger choose Pub / Sub and select the topic from the previous step.</p>
<p>How mine looks:</p>
<p><img alt="Cloud Function" src="../../../../images/cloud_func.png" />
Configure the following environment variables with your function:</p>
<ul>
<li><code>OWNER</code>: The repo owner of the repo where the workflow will be triggered. github.com/\&lt;owner&gt;/\&lt;repo name&gt;</li>
<li><code>REPO</code>: The repo name github.com/\&lt;owner&gt;/\&lt;repo name&gt;</li>
<li><code>EVENT_TYPE</code>: I used 'webhook-gcloud' so that when the actions workflow runs, it will show as this type of trigger.</li>
<li><code>DB_SCHEMA</code>: This will be the eventual S3 bucket path e.g 'dev' while testing then 'production' when up and running.</li>
</ul>
<p>Over in Github, create a personal access token and store it in Google Cloud as a secret called <code>PAT</code>. This should be a Github access token which has permission to read and write to the repo. Go to <a href="https://github.com/settings/personal-access-tokens">developer settings</a> in Github and create a new fine grained access token and give it appropriate permissions.</p>
<p><img alt="Fine Grained Access Token" src="../../../../images/fine_grained_access_settings.png" /></p>
<p>Back in Google Cloud, your cloud function variables tab should look similar to this:</p>
<p><img alt="Cloud Function Vars" src="../../../../images/cloud_func_vars.png" /></p>
<p>Next we need a function to tell the Github Actions workflow that new data are available for a specific date. The <code>DATE_PART</code> comes from the event data that comes with the pub / sub topic trigger. You can paste code directly into a cloud function or sync with a repo. I used the following Python for the function.</p>
<pre><code># Triggers a github actions workflow using an api request
# Test via the functions testing tab in GCP functions UI

import requests
import os
import base64
import json
from datetime import datetime

OWNER = os.environ['OWNER']
REPO = os.environ['REPO']
EVENT_TYPE = os.environ['EVENT_TYPE']
PAT = os.environ['PAT']
DB_SCHEMA = os.environ['DB_SCHEMA'] # will be a path in the S3 bucket
DATE_PART = '2022-12-01' # default for testing, will be overwritten within funcs

# funcs
## api request
def requestGHA(DATE_PART, PAT=PAT, OWNER=OWNER, REPO=REPO, EVENT_TYPE=EVENT_TYPE):

    # Set up the headers for the API request
    headers = {
        &quot;Accept&quot;: &quot;application/vnd.github+json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {PAT}&quot;,
    }

    # Set up the URL for the API request
    url = f&quot;https://api.github.com/repos/{OWNER}/{REPO}/dispatches&quot;

    # Set up the payload for the API request
    payload = {
        &quot;event_type&quot;: EVENT_TYPE,
        &quot;client_payload&quot;: {
            &quot;start_date&quot;: DATE_PART,
            &quot;end_date&quot;: DATE_PART,
            &quot;db_schema&quot;: DB_SCHEMA
        }
    }

    # Make the API request
    response = requests.post(url, json=payload, headers=headers)

    # Print the response status code
    return(str(response.status_code))

def callRequest(event, context): # on cloud pub sub event will trigger with these 2 args

    global DATE_PART

    # if the function is called from the pub/sub trigger get the date, otherwise during testing use the default DATE_PART from globals above
    if 'data' in event:

        event_data = base64.b64decode(event['data']).decode('utf-8')
        event_data = json.loads(event_data) # turn from a string to a dict
        destination_table_id = event_data['protoPayload']['serviceData']['jobCompletedEvent']['job']['jobConfiguration']['load']['destinationTable']['tableId']
        print(&quot;destination_table_id&quot;)
        print(destination_table_id)

        DATE_PART = destination_table_id.split('_')[1]
        DATE_PART = datetime.strptime(DATE_PART, '%Y%m%d')
        DATE_PART = DATE_PART.strftime('%Y-%m-%d')

    print(&quot;--- here is date part ---&quot;)
    print(DATE_PART)

    st = requestGHA(DATE_PART=DATE_PART)
    return(st)
</code></pre>
<p>You can test the function in the GCP UI, you should get a 'OK' message like in the image below. Later, once we create an actions workflow file in your repo, you will be able to see the 'webhook-gcloud' event trigger in the repo actions tab.</p>
<p>The function pings your repo via an http request and sends the event date along with it. This will be used in the workflow to extract data from Big Query and to then send along to AWS S3 via Google Storage.</p>
<p><img alt="Cloud Function Test OK" src="../../../../images/test_ok.png" /></p>
<p>Once Github Actions are set up and authorization is done (steps are further down), you should also see a screen within Github Actions like this when testing the workflow:</p>
<p><img alt="Cloud Function Test OK" src="../../../../images/test_webhook_gha.png" /></p>
<h2 id="rsync-data-between-google-storage-and-s3">Rsync data between Google Storage and S3</h2>
<p>The workflow file in this example should be stored in your repo at <code>.github/workflows/main.yml</code>.</p>
<p>First set some repo secrets:</p>
<ul>
<li><code>AWS_SECRET_ACCESS_KEY</code></li>
<li><code>AWS_ACCESS_KEY_ID</code></li>
<li><code>AWS_S3_BUCKET</code> The name of the bucket where you will store the extracted data</li>
<li><code>GS_BUCKET</code> The name of the Google Storage bucket that will be used to save the Big Query data before Rsync. Example form <code>gs://ga4-extract-bigquery</code></li>
<li><code>SERVICE_JSON</code> The service account json that will be used to authorize the gcloud cli and access both Big Query and Google Storage. Ensure appropriate permissions.</li>
<li><code>GCP_PROJECT</code> The name of the Google Cloud project where you extract your GA4 data to</li>
</ul>
<p>Then, define some variables from the incoming http request, namely the table date of the data to be extracted.</p>
<pre><code>name: GA Extract

on:
  workflow_dispatch:
    inputs:
      start_date:
        required: true
        type: string
        default: '2022-12-01'
      end_date:
        required: true
        type: string
        default: '2022-12-01'
      db_schema:
        required: true
        type: string
        default: 'dev'  
  repository_dispatch:
    types: [webhook-gcloud] 

env:
  DB_SCHEMA: ${{ github.event.client_payload.db_schema || inputs.db_schema }}
  START_DATE: ${{ github.event.client_payload.start_date || inputs.start_date }}
  END_DATE: ${{ github.event.client_payload.end_date || inputs.end_date }}
</code></pre>
<p>The workflow dispatch trigger is for the when the workflow is run manually via the actions UI. </p>
<p>When the workflow is triggered via the Google cloud function, variables <code>start_date</code>, <code>end_date</code> and <code>db_schema</code> are taken from the http request payload from the python via Google cloud above and set as env vars for the workflow. A trigger is set up for API requests using <code>repository_dispatch</code> where the type matches what we set in the cloud functions env vars <code>webhook-gcloud</code>.</p>
<p>In case the workflow is run manually while developing or if back filling, <code>start_date</code> and <code>end_date</code> and <code>db_schema</code> are set using variable expansion (<code>FOO = this || that</code>).</p>
<p>Now that the environment is set, define the job then checkout and authorize Google's gcloud actions container:</p>
<pre><code>jobs:
  get-bq-data:
    runs-on: ubuntu-22.04
    name: BQ-GS-S3
    steps:
      - name: Checkout current repo
        uses: actions/checkout@v2 # Defaults to current repo
      - name: Auth
        uses: 'google-github-actions/auth@v0'
        with:
          credentials_json: '${{ secrets.SERVICE_JSON }}'
      - name: Set up Cloud SDK
        uses: 'google-github-actions/setup-gcloud@v0'
</code></pre>
<p>Finally, Rsync between Google Storage and AWS S3. Usually the workflow will trigger for a single table suffix' date, however since we are using Github Actions we can use the workflow to back fill data over a date range, so I used a loop.</p>
<p>Note the placeholder \&lt;events schema name&gt;. This should be the schema where GA4 data are exported to in Big Query. It might look something like <code>analytics_1234567</code> (Some other non sequential number after <code>analytics_</code>). Add your schema name there.</p>
<pre><code>      - name: Run extract and save data
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: 'us-west-2'
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          GCP_PROJECT: ${{ secrets.GCP_PROJECT }}
          GS_BUCKET: ${{ secrets.GS_BUCKET }}
        run: |

          # config
          gcloud config set project $GCP_PROJECT

          # get rid of welcome message in loop below
          bq query select 1 &gt; /dev/null

          # set rundate and vars
          start_date=${{ env.START_DATE}}
          end_date=${{ env.END_DATE }}
          start_date=$(date -d &quot;$start_date&quot; +&quot;%Y%m%d&quot;)
          end_date=$(date -d &quot;$end_date&quot; +&quot;%Y%m%d&quot;)
          DB_SCHEMA=${{ env.DB_SCHEMA }}

          # confirm date range for extract
          echo &quot;Extracting Google Analytics data for between $start_date and $end_date&quot;

          # extract over date range
          until [[ $start_date &gt; $end_date ]]; do

              echo &quot;extracting for data on $start_date&quot;

              # save extract to google storage
              bq extract \
              --destination_format NEWLINE_DELIMITED_JSON \
              '&lt;events schema name&gt;.events_'$start_date \ # add actual schema name here
              $GS_BUCKET/daily-extracts/_$start_date/data-*.json

              # check data are present in gs
              gsutil ls $GS_BUCKET/daily-extracts/_$start_date

              # rsync from gs to aws
              gsutil -m rsync -r $GS_BUCKET/daily-extracts/ s3://${{ secrets.AWS_S3_BUCKET }}/${{ env.DB_SCHEMA }}

              # clear out gs before backfilling
              gsutil rm -r $GS_BUCKET/daily-extracts/_$start_date

              # increment
              start_date=$(date -d &quot;$start_date + 1 day&quot; +&quot;%Y%m%d&quot;)

          done

          # check data are in aws s3 after loop
          aws s3 ls ${{ secrets.AWS_S3_BUCKET }}/${{ env.DB_SCHEMA }} --recursive
</code></pre>
<p>The above 3 code blocks are <code>.github/workflows/main.yml</code>.</p>
<p>Data are extracted using gsutil to Google Storage using <code>bq extract</code>. The data are Rsynced from Google Storage to AWS S3.</p>
<p>When GA4 data are updated in Big Query and the workflow triggers you should see the webhook_gcloud event in the actions tab, like this:</p>
<p><img alt="Actions workflow run via webhook" src="../../../../images/webhook_gcloud.png" />
Notice how there are several updates within any 24 hour period. GA4 data are updated several times a day.</p>
<p>You should now be able ingest this json into your db engine of choice. One approach is using a lambda function with a listener on updates to the S3 bucket. The following example schema for Redshift mirrors the Big Query schema:</p>
<pre><code>CREATE TABLE IF NOT EXISTS ga4_extract.&quot;daily-ga4-raw&quot;
(
    event_date DATE   ENCODE az64
    ,event_timestamp BIGINT   ENCODE az64
    ,event_name VARCHAR(256)   ENCODE lzo
    ,event_params SUPER   ENCODE zstd
    ,event_previous_timestamp BIGINT   ENCODE az64
    ,event_value_in_usd NUMERIC(18,0)   ENCODE az64
    ,event_bundle_sequence_id INTEGER   ENCODE az64
    ,event_server_timestamp_offset INTEGER   ENCODE az64
    ,user_id VARCHAR(256)   ENCODE lzo
    ,user_pseudo_id VARCHAR(256)   ENCODE lzo
    ,privacy_info SUPER   ENCODE zstd
    ,user_properties SUPER   ENCODE zstd
    ,user_first_touch_timestamp BIGINT   ENCODE az64
    ,user_ltv SUPER   ENCODE zstd
    ,device SUPER   ENCODE zstd
    ,geo SUPER   ENCODE zstd
    ,app_info SUPER   ENCODE zstd
    ,traffic_source SUPER   ENCODE zstd
    ,stream_id VARCHAR(256)   ENCODE lzo
    ,platform VARCHAR(256)   ENCODE lzo
    ,event_dimensions SUPER   ENCODE zstd
    ,ecommerce SUPER   ENCODE zstd
    ,items SUPER   ENCODE zstd
)
DISTSTYLE EVEN
;
</code></pre>
<p>The above table should be able to store the exported json from Big Query. <a href="https://support.google.com/analytics/answer/7029846?hl=en">Here is the extract event schema documentation</a>.</p>







  
  



  


  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": [], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
    
  </body>
</html>